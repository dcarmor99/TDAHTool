{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "3CRCNoXI2KEj",
        "outputId": "c2f50685-b0fd-43a7-9816-d6f00a409584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1408506528>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importaci√≥n de librerias necesarias"
      ],
      "metadata": {
        "id": "CMaU7CSk2i76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4toSN_8J1FgQ"
      },
      "outputs": [],
      "source": [
        "# 1. Importaci√≥n de librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga del conjunto de datos 'Train'"
      ],
      "metadata": {
        "id": "gr8ljOyp2qrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "processed_path_train = '/content/drive/MyDrive/Colab Notebooks/Proyecto-TFG/data/train.csv'\n",
        "train_df = pd.read_csv(processed_path_train)\n",
        "print(\"Cargado conjunto de entrenamiento. Dimensiones:\", train_df.shape)"
      ],
      "metadata": {
        "id": "ckPM9b232o7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lista de variables que queremos identificar\n"
      ],
      "metadata": {
        "id": "q0z0StJY3nWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables = {\n",
        "    \"Child's age\": \"sc_age_years\",\n",
        "    \"Sex\": \"sc_sex\",\n",
        "    \"Mother's age\": \"a1_age\",\n",
        "    \"Allergies\": \"allergies\",\n",
        "    \"Arthritis\": \"autoimmune\",\n",
        "    \"Ashtma\": \"k2q40a\",\n",
        "    \"Brain injury\": \"concussion\",\n",
        "    \"Headaches\": \"headache\",\n",
        "    \"Anxiety\": \"anxiety\",\n",
        "    \"Depression\": \"k2q32a\",\n",
        "    \"Insurance\": \"currcov\",\n",
        "    \"Alcohol\": \"ace9\",\n",
        "    \"Race\": [\"sc_racer\", \"sc_race_r\"],  # Dos posibles nombres\n",
        "    \"Family structure\": \"family_r\",\n",
        "    \"Mother's education\": \"higrade\",\n",
        "    \"Very LBW - LBW\": \"birthwt\",\n",
        "    \"Poverty\": \"fpl_i1\"\n",
        "}"
      ],
      "metadata": {
        "id": "ruAyt_xt3mQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar si las variables est√°n en el dataset y mostrar valores √∫nicos\n",
        "for var_name, col in variables.items():\n",
        "    if isinstance(col, list):  # Si hay m√°s de un posible nombre (ej. \"Race\")\n",
        "        found = False\n",
        "        for c in col:\n",
        "            if c in train_df.columns:\n",
        "                found = True\n",
        "                print(f\"‚úÖ '{var_name}' est√° en el dataset como '{c}'\")\n",
        "                print(f\"   Valores √∫nicos: {train_df[c].unique()}\\n\")\n",
        "                break\n",
        "        if not found:\n",
        "            print(f\"‚ùå '{var_name}' no est√° en el dataset\\n\")\n",
        "    else:\n",
        "        if col in train_df.columns:\n",
        "            print(f\"‚úÖ '{var_name}' est√° en el dataset como '{col}'\")\n",
        "            print(f\"   Valores √∫nicos: {train_df[col].unique()}\\n\")\n",
        "        else:\n",
        "            print(f\"‚ùå '{var_name}' no est√° en el dataset\\n\")"
      ],
      "metadata": {
        "id": "uOpAqvi-64_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variables_articulo = [\n",
        "    \"sc_age_years\", \"sc_sex\", \"a1_age\", \"allergies\", \"autoimmune\", \"k2q40a\",\n",
        "    \"concussion\", \"headache\", \"k2q32a\", \"ace9\",\n",
        "    \"sc_racer\", \"family_r\", \"higrade\", \"birthwt\", \"fpl_i1\"\n",
        "]\n",
        "\n",
        "# Variables presentes en el dataset\n",
        "variables_presentes = [var for var in variables_articulo if var in train_df.columns]\n",
        "\n",
        "# Variables que faltan\n",
        "variables_faltantes = [var for var in variables_articulo if var not in train_df.columns]\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Variables presentes en el dataset:\")\n",
        "print(variables_presentes)\n",
        "\n",
        "print(\"\\nVariables faltantes en el dataset:\")\n",
        "print(variables_faltantes)\n"
      ],
      "metadata": {
        "id": "R4iKGTb17LU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizamos los datos\n",
        "for col in train_df.columns:\n",
        "    print(f\"\\nColumna: {col}\")\n",
        "    print(train_df[col].unique())\n"
      ],
      "metadata": {
        "id": "pxVFTqgel7M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tratamiento rango de valores num√©ricos**"
      ],
      "metadata": {
        "id": "zfvUVlUZl-i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def limpiar_y_convertir_a_int(train_df, columnas):\n",
        "    \"\"\"\n",
        "    Limpia y convierte columnas con valores como '50 or more' o '40 or less' a enteros,\n",
        "    manteniendo NaNs, usando el tipo pandas 'Int64'.\n",
        "    \"\"\"\n",
        "    def extraer_entero(valor):\n",
        "        if isinstance(valor, str):\n",
        "            match = re.match(r\"(\\d+)\", valor)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "            else:\n",
        "                return np.nan\n",
        "        elif isinstance(valor, (int, float)) and not pd.isna(valor):\n",
        "            return int(valor)\n",
        "        else:\n",
        "            return np.nan\n",
        "\n",
        "    for col in columnas:\n",
        "        if col in train_df.columns:\n",
        "            print(f\"‚Üí Limpiando y convirtiendo '{col}'...\")\n",
        "            train_df[col] = train_df[col].apply(extraer_entero).astype(\"Int64\")\n",
        "\n",
        "    return train_df\n"
      ],
      "metadata": {
        "id": "nfZ2-Sm7QNq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_cadenas_a_enteros(train_df, columnas):\n",
        "    \"\"\"\n",
        "    Convierte valores como '2009' (strings num√©ricos) a enteros,\n",
        "    dejando como NaN los que no se pueden convertir.\n",
        "    \"\"\"\n",
        "    for col in columnas:\n",
        "        if col in train_df.columns:\n",
        "            print(f\"‚Üí Convirtiendo strings num√©ricos en '{col}' a enteros...\")\n",
        "            train_df[col] = pd.to_numeric(train_df[col], errors='coerce').astype('Int64')\n",
        "    return train_df"
      ],
      "metadata": {
        "id": "Yk4aQNieRyJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corregimos tipos de datos numericos"
      ],
      "metadata": {
        "id": "rSqLCFVcR7Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_a_corregir = ['birth_yr', 'a1_age', 'fpl_i1']  # O cualquier otra columna con a√±os como strings\n",
        "train_df = convertir_cadenas_a_enteros(train_df, columnas_a_corregir)\n"
      ],
      "metadata": {
        "id": "CT2kh1fSR9zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columnas que quieres limpiar y convertir a enteros\n",
        "columnas_a_tratar = ['fpl_i1', 'a1_age', 'sc_age_years']  # A√±ade las que quieras\n",
        "\n",
        "# Aplicar la limpieza\n",
        "train_df = limpiar_y_convertir_a_int(train_df, columnas_a_tratar)\n"
      ],
      "metadata": {
        "id": "PW00KwnXSOq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train_df.columns:\n",
        "    print(f\"\\nColumna: {col}\")\n",
        "    print(train_df[col].unique())\n"
      ],
      "metadata": {
        "id": "fYNDmFdRQPBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudio de valores nulos"
      ],
      "metadata": {
        "id": "8VX-3QVv8CHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Estudio de valores nulos\n",
        "nulos_por_columna = train_df.isnull().sum()\n",
        "no_nulos_por_columna = train_df.notnull().sum()\n",
        "\n",
        "resumen_nulos = pd.DataFrame({\n",
        "    'Columna': train_df.columns,\n",
        "    'Valores Nulos': nulos_por_columna,\n",
        "    'Valores No Nulos': no_nulos_por_columna\n",
        "})\n",
        "\n",
        "\n",
        "# Permitir que pandas muestre todas las filas\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Mostrar el DataFrame con todas las filas\n",
        "print(resumen_nulos)"
      ],
      "metadata": {
        "id": "OyBNASrwS_sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analizar la cantidad de valores nulos\n",
        "# Calcular valores nulos por columna\n",
        "missing_values = train_df.isnull().sum()\n",
        "\n",
        "# Calcular porcentaje de valores nulos\n",
        "missing_percentage = (missing_values / len(train_df)) * 100\n",
        "\n",
        "# Crear un DataFrame resumen\n",
        "missing_data = pd.DataFrame({'Valores Nulos': missing_values, '% de Nulos': missing_percentage})\n",
        "missing_data = missing_data.sort_values(by='Valores Nulos', ascending=False)\n",
        "\n",
        "print(missing_data)\n"
      ],
      "metadata": {
        "id": "aokm64TuTG0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliminamos repetidos"
      ],
      "metadata": {
        "id": "XLG-qGgvTXGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos columnas repetidas\n",
        "train_df = train_df.drop(columns=['cerpals_desc.1'])\n",
        "train_df = train_df.drop(columns=['k2q31c.1'])\n",
        "train_df = train_df.drop(columns=['k2q40a.1'])"
      ],
      "metadata": {
        "id": "QLr8BZzdTO56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train_df.columns:\n",
        "    print(f\"\\nColumna: {col}\")\n",
        "    print(train_df[col].unique())"
      ],
      "metadata": {
        "id": "vqU4q_3VzVTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tama√±o del DataFrame\n",
        "print(\"üìê Shape del DataFrame (filas, columnas):\")\n",
        "print(train_df.shape)\n",
        "\n",
        "# N√∫mero de filas y columnas por separado\n",
        "print(f\"\\nüìÑ N√∫mero de filas: {train_df.shape[0]}\")\n",
        "print(f\"üìä N√∫mero de columnas: {train_df.shape[1]}\")\n",
        "\n",
        "# Conteo de tipos de datos\n",
        "print(\"\\nüìö Tipos de datos en el DataFrame:\")\n",
        "print(train_df.dtypes.value_counts())\n",
        "\n",
        "# Columnas que todav√≠a tienen texto (tipo object)\n",
        "object_cols = [col for col in train_df.columns if train_df[col].dtype == 'object']\n",
        "print(f\"\\nüü® Columnas tipo 'object' (texto): {len(object_cols)}\")\n",
        "print(object_cols)\n",
        "\n",
        "# Columnas con valores nulos\n",
        "columnas_con_nulos = train_df.columns[train_df.isnull().any()]\n",
        "print(f\"\\nüíß Columnas con valores nulos: {len(columnas_con_nulos)}\")\n",
        "print(columnas_con_nulos.tolist())\n",
        "\n",
        "# Total de valores nulos en el dataset\n",
        "total_nulos = train_df.isnull().sum().sum()\n",
        "print(f\"\\nüî¢ Total de valores nulos en todo el dataset: {total_nulos}\")\n",
        "\n",
        "# Porcentaje de valores nulos por columna\n",
        "print(\"\\nüìâ Porcentaje de valores nulos por columna (si hay):\")\n",
        "porcentaje_nulos = train_df.isnull().mean() * 100\n",
        "print(porcentaje_nulos[porcentaje_nulos > 0].sort_values(ascending=False))\n",
        "\n",
        "# Columnas con una sola categor√≠a (no aportan al modelo)\n",
        "columnas_sin_variabilidad = train_df.columns[train_df.nunique(dropna=True) <= 1]\n",
        "print(f\"\\n‚ö†Ô∏è Columnas con solo una categor√≠a: {len(columnas_sin_variabilidad)}\")\n",
        "print(columnas_sin_variabilidad.tolist())\n"
      ],
      "metadata": {
        "id": "XdeLl-EkaO3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ========== Diccionario de tipos de variables ==========\n",
        "variables_tipos = {\n",
        "    \"k7q84_r\": \"Ordinal\", \"hardwork\": \"Ordinal\", \"k8q34\": \"Ordinal\", \"k8q32\": \"Ordinal\",\n",
        "    \"k8q31\": \"Ordinal\", \"wktosolve\": \"Ordinal\", \"talkabout\": \"Ordinal\", \"sharetoys\": \"Ordinal\",\n",
        "    \"waitforturn\": \"Ordinal\", \"outdoorswkend\": \"Ordinal\", \"calmdown_r\": \"Ordinal\", \"temper_r\": \"Ordinal\",\n",
        "    \"nameemotions\": \"Ordinal\", \"focuson\": \"Ordinal\", \"bullied_r\": \"Ordinal\", \"k7q70_r\": \"Ordinal\",\n",
        "    \"k7q83_r\": \"Ordinal\", \"outdoorswkday\": \"Ordinal\", \"screentime\": \"Ordinal\", \"k7q85_r\": \"Ordinal\",\n",
        "    \"sc_k2q22\": \"Binaria\", \"higrade\": \"Ordinal\", \"birth_yr_f\": \"Categ√≥rica\", \"birthwt\": \"Categ√≥rica\",\n",
        "    \"birthwt_l\": \"Binaria\", \"family_r\": \"Categ√≥rica\", \"sc_racer\": \"Categ√≥rica\", \"agepos4\": \"Ordinal\",\n",
        "    \"sc_english\": \"Ordinal\", \"hcability\": \"Ordinal\", \"grades\": \"Categ√≥rica\", \"makefriend\": \"Ordinal\",\n",
        "    \"k3q04_r\": \"Categ√≥rica\", \"bedtime\": \"Ordinal\", \"k7q33\": \"Ordinal\", \"k8q30\": \"Ordinal\",\n",
        "    \"k8q21\": \"Ordinal\", \"k7q82_r\": \"Ordinal\", \"fpl_i1\": \"Num√©rica\", \"k6q73_r\": \"Ordinal\",\n",
        "    \"k6q70_r\": \"Ordinal\", \"a1_menthealth\": \"Ordinal\", \"a1_physhealth\": \"Ordinal\", \"k6q71_r\": \"Ordinal\",\n",
        "    \"k7q04r_r\": \"Ordinal\", \"a1_relation\": \"Categ√≥rica\", \"a1_marital\": \"Categ√≥rica\", \"k2q33b\": \"Binaria\",\n",
        "    \"k2q32b\": \"Binaria\", \"k2q34b\": \"Binaria\", \"k2q31b\": \"Binaria\", \"k2q31d\": \"Binaria\", \"addtreat\": \"Binaria\",\n",
        "    \"confirminjury\": \"Binaria\", \"sescurrsvc\": \"Binaria\", \"cerpals_desc\": \"Ordinal\", \"k2q34c\": \"Ordinal\",\n",
        "    \"k2q31c\": \"Ordinal\", \"hhlanguage\": \"Categ√≥rica\", \"birth_yr\": \"Categ√≥rica\", \"a1_age\": \"Num√©rica\",\n",
        "    \"memorycond\": \"Binaria\", \"allergies\": \"Binaria\", \"k2q40a\": \"Binaria\", \"autoimmune\": \"Binaria\",\n",
        "    \"headache\": \"Binaria\", \"k2q33a\": \"Binaria\", \"k2q32a\": \"Binaria\", \"k2q34a\": \"Binaria\", \"k2q31a\": \"Binaria\",\n",
        "    \"concussion\": \"Binaria\", \"k4q23\": \"Binaria\", \"k6q15\": \"Binaria\", \"ace9\": \"Binaria\", \"a1_sex\": \"Categ√≥rica\",\n",
        "    \"a1_born\": \"Categ√≥rica\", \"sc_age_years\": \"Ordinal\", \"sc_sex\": \"Categ√≥rica\", \"a1_grade\": \"Categ√≥rica\"\n",
        "}\n",
        "\n",
        "# ========== Diccionario de mapeos ==========\n",
        "mapeos = {}\n",
        "\n",
        "# ========== Funci√≥n de mapeo ==========\n",
        "def generar_mapeo(columna, tipo):\n",
        "    if tipo == \"Binaria\":\n",
        "        return {\n",
        "            \"Yes\": 1, \"No\": 0,\n",
        "            \"No valid response\": np.nan, \"Logical skip\": -1, \"Not in universe\": np.nan\n",
        "        }\n",
        "    else:\n",
        "        valores_unicos = columna.dropna().unique()\n",
        "        mapeo = {valor: idx + 1 for idx, valor in enumerate(sorted(valores_unicos))}\n",
        "        mapeo.update({\"Logical skip\": -1, \"Not in universe\": np.nan, \"No valid response\": np.nan})\n",
        "        return mapeo\n",
        "\n",
        "# ========== Mapear solo variables no num√©ricas ==========\n",
        "def mapear_dataset(train_df, variables_tipos):\n",
        "    for columna, tipo in variables_tipos.items():\n",
        "        if columna in train_df.columns and tipo != \"Num√©rica\":\n",
        "            print(f\"Mapeando columna: {columna}\")\n",
        "            mapeo = generar_mapeo(train_df[columna], tipo)\n",
        "            mapeos[columna] = mapeo\n",
        "            train_df[columna] = train_df[columna].map(mapeo)\n",
        "    return train_df\n",
        "\n",
        "# ========== USO ==========\n",
        "# train_df debe estar previamente cargado\n",
        "train_df = mapear_dataset(train_df, variables_tipos)\n",
        "\n",
        "# ========== Revisi√≥n ==========\n",
        "print(\"\\n‚úÖ Dataset tras mapeo a valores num√©ricos:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nüìä Tipos de datos despu√©s del mapeo:\")\n",
        "print(train_df.dtypes)\n"
      ],
      "metadata": {
        "id": "DuPfjvg5npa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for columna, mapa in mapeos.items():\n",
        "    print(f\"\\nüìå Columna: {columna}\")\n",
        "    for clave, valor in mapa.items():\n",
        "        print(f\"  '{clave}' ‚Üí {valor}\")"
      ],
      "metadata": {
        "id": "_s26d6d6zrWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train_df.columns:\n",
        "    print(f\"\\nColumna: {col}\")\n",
        "    print(train_df[col].unique())"
      ],
      "metadata": {
        "id": "wLyNwjuJn6hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def knn_univariado_imputation(train_df, variables_tipos, mapeos, n_neighbors=5):\n",
        "    \"\"\"\n",
        "    Imputaci√≥n univariada KNN por columna. Cada columna se imputa por sus propios vecinos m√°s cercanos.\n",
        "    \"\"\"\n",
        "    df_imputado = train_df.copy()\n",
        "\n",
        "    for col in df_imputado.columns:\n",
        "        if df_imputado[col].isnull().sum() == 0:\n",
        "            continue  # Saltar columnas sin nulos\n",
        "\n",
        "        print(f\"üîß Imputando columna: {col}\")\n",
        "        tipo = variables_tipos.get(col, \"Num√©rica\")\n",
        "        valores_col = df_imputado[col].values\n",
        "\n",
        "        # √çndices de valores nulos y v√°lidos\n",
        "        idx_nulos = np.where(pd.isna(valores_col))[0]\n",
        "        idx_validos = np.where(~pd.isna(valores_col))[0]\n",
        "\n",
        "        for idx in idx_nulos:\n",
        "            vecinos = valores_col[idx_validos]\n",
        "\n",
        "            # Si hay menos vecinos v√°lidos que K, usar todos\n",
        "            if len(vecinos) == 0:\n",
        "                imputado = 0 if tipo == \"Num√©rica\" else -1  # fallback\n",
        "            else:\n",
        "                vecinos_mas_cercanos = vecinos[:n_neighbors]  # no hay orden real, usamos primeros K\n",
        "\n",
        "                if tipo == \"Num√©rica\":\n",
        "                    imputado = np.nanmean(vecinos_mas_cercanos)\n",
        "                    # Si la columna original es Int64, redondeamos\n",
        "                    if pd.api.types.is_integer_dtype(train_df[col].dtype):\n",
        "                        imputado = int(round(imputado))\n",
        "                else:\n",
        "                    imputado = Counter(vecinos_mas_cercanos).most_common(1)[0][0]\n",
        "\n",
        "            df_imputado.at[idx, col] = imputado\n",
        "\n",
        "        # Validaci√≥n con mapeo\n",
        "        if tipo in [\"Binaria\", \"Ordinal\", \"Categ√≥rica\"] and col in mapeos:\n",
        "            valores_validos = set(v for v in mapeos[col].values() if not pd.isna(v))\n",
        "            df_imputado[col] = df_imputado[col].apply(\n",
        "                lambda x: min(valores_validos, key=lambda v: abs(v - x)) if x not in valores_validos else x\n",
        "            )\n",
        "            df_imputado[col] = df_imputado[col].astype(int)\n",
        "\n",
        "    print(\"‚úÖ Imputaci√≥n univariada KNN finalizada.\")\n",
        "    return df_imputado\n"
      ],
      "metadata": {
        "id": "lfp0FwBWa17r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = knn_univariado_imputation(train_df, variables_tipos, mapeos)\n",
        "\n",
        "# Guardar el DataFrame limpio en Google Drive\n",
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/Proyecto-TFG/data/1_train_df_clean.csv\"\n",
        "train_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Dataset guardado en: {output_path}\")"
      ],
      "metadata": {
        "id": "Ga7MpJOk8u5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_path = \"/content/drive/MyDrive/Colab Notebooks/Proyecto-TFG/data/1_train_df_clean.csv\"\n",
        "c_df = pd.read_csv(clean_path)"
      ],
      "metadata": {
        "id": "0c-KCPMiKn8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Volvemos a comprobar si hay nulos\n",
        "nulos_por_columna = c_df.isnull().sum()\n",
        "no_nulos_por_columna = c_df.notnull().sum()\n",
        "\n",
        "resumen_nulos = pd.DataFrame({\n",
        "    'Columna': c_df.columns,\n",
        "    'Valores Nulos': nulos_por_columna,\n",
        "    'Valores No Nulos': no_nulos_por_columna\n",
        "})\n",
        "\n",
        "\n",
        "# Permitir que pandas muestre todas las filas\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Mostrar el DataFrame con todas las filas\n",
        "print(resumen_nulos)"
      ],
      "metadata": {
        "id": "FWjzjZAvegsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in c_df.columns:\n",
        "    print(f\"\\nColumna: {col}\")\n",
        "    print(train_df[col].unique())"
      ],
      "metadata": {
        "id": "Vx3LKxT-VHu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspecci√≥n general de todas las columnas en train_df\n",
        "for col in c_df.columns:\n",
        "    print(f\"üìå Columna: {col}\")\n",
        "    print(f\"üî∏ Valores √∫nicos: {c_df[col].unique()}\")\n",
        "    print(f\"üî∏ Cantidad de valores √∫nicos: {c_df[col].nunique(dropna=True)}\")\n",
        "    print(f\"üî∏ Porcentaje de nulos: {c_df[col].isnull().mean() * 100:.2f}%\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "DVY3i9j41oTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Desmapear para obtener valores originales limpios**"
      ],
      "metadata": {
        "id": "gK7Iu13w2Onj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def desmapear_dataset(c_df, mapeos, variables_tipos):\n",
        "    \"\"\"\n",
        "    Reemplaza valores num√©ricos por sus valores originales seg√∫n los mapeos invertidos.\n",
        "    \"\"\"\n",
        "    df_desmapeado = c_df.copy()\n",
        "\n",
        "    for col, tipo in variables_tipos.items():\n",
        "        if col in df_desmapeado.columns and col in mapeos and tipo != \"Num√©rica\":\n",
        "            # Crear mapeo inverso: valor_num√©rico -> valor_original\n",
        "            mapeo_original = mapeos[col]\n",
        "            mapeo_inverso = {v: k for k, v in mapeo_original.items() if pd.notna(v)}\n",
        "\n",
        "            print(f\"üîÅ Desmapeando columna: {col}\")\n",
        "            df_desmapeado[col] = df_desmapeado[col].map(mapeo_inverso)\n",
        "\n",
        "    return df_desmapeado\n",
        "\n"
      ],
      "metadata": {
        "id": "XToSQ7502Tsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_df = desmapear_dataset(c_df, mapeos, variables_tipos)"
      ],
      "metadata": {
        "id": "FaS0HoPQ2U0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in c_df.columns:\n",
        "    print(f\"\\nColumna: {col}\")\n",
        "    print(c_df[col].unique())\n"
      ],
      "metadata": {
        "id": "rNFRou-PY04p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Volvemos a comprobar si hay nulos\n",
        "nulos_por_columna = c_df.isnull().sum()\n",
        "no_nulos_por_columna = c_df.notnull().sum()\n",
        "\n",
        "resumen_nulos = pd.DataFrame({\n",
        "    'Columna': c_df.columns,\n",
        "    'Valores Nulos': nulos_por_columna,\n",
        "    'Valores No Nulos': no_nulos_por_columna\n",
        "})\n",
        "\n",
        "\n",
        "# Permitir que pandas muestre todas las filas\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Mostrar el DataFrame con todas las filas\n",
        "print(resumen_nulos)"
      ],
      "metadata": {
        "id": "2iVSIE2ofMYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame limpio y desmapeado en Google Drive\n",
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/Proyecto-TFG/data/2_train_df_clean.csv\"\n",
        "c_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Dataset guardado en: {output_path}\")"
      ],
      "metadata": {
        "id": "IaH75QUaLz97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}